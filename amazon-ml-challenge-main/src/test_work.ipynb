{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b70b34e",
   "metadata": {},
   "source": [
    "### Basic library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911e33",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3136aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "\n",
    "#sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "#sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a205ffd-7225-4f5b-b6d6-d9e21667a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'depth': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'height': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'item_weight': {\n",
    "        'gram', 'g', 'gm', \n",
    "        'kilogram', 'kg', \n",
    "        'microgram', 'µg', \n",
    "        'milligram', 'mg', \n",
    "        'ounce', 'oz', \n",
    "        'pound', 'lb', \n",
    "        'ton', 't'\n",
    "    },\n",
    "    'maximum_weight_recommendation': {\n",
    "        'gram', 'g', 'gm', \n",
    "        'kilogram', 'kg', \n",
    "        'microgram', 'µg', \n",
    "        'milligram', 'mg', \n",
    "        'ounce', 'oz', \n",
    "        'pound', 'lb', \n",
    "        'ton', 't'\n",
    "    },\n",
    "    'voltage': {'kilovolt', 'kV', 'millivolt', 'mV', 'volt', 'V'},\n",
    "    'wattage': {'kilowatt', 'kW', 'watt', 'W'},\n",
    "    'item_volume': {\n",
    "        'centilitre', 'cL', \n",
    "        'cubic foot', 'ft³', \n",
    "        'cubic inch', 'in³', \n",
    "        'cup', 'c', \n",
    "        'decilitre', 'dL', \n",
    "        'fluid ounce', 'fl oz', \n",
    "        'gallon', 'gal', \n",
    "        'imperial gallon', 'imp gal', \n",
    "        'litre', 'L', \n",
    "        'microlitre', 'µL', \n",
    "        'millilitre', 'mL', \n",
    "        'pint', 'pt', \n",
    "        'quart', 'qt'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd689",
   "metadata": {},
   "source": [
    "### Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bb3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Filepath: ../dataset/sample_test.csv invalid or not found.\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa79459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Filepath: ../dataset/sample_test.csv invalid or not found.\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out_fail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe930a8",
   "metadata": {},
   "source": [
    "### Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d1aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from utils import download_images\\ndownload_images(t['image_link'], '../images')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from utils import download_images\n",
    "download_images(t['image_link'], '../images')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89aaba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert len(os.listdir('../images')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba3d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ../images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7cc5e-40aa-4c1e-ba19-1b0a530f0cf4",
   "metadata": {},
   "source": [
    "## Mapping url to its file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9ad06-8584-41d2-82d8-925bcff0d076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140743c9-3d3a-45b4-99b2-e52b66f98262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_link   image_filename\n",
      "0  https://m.media-amazon.com/images/I/110EibNycl...  110EibNyclL.jpg\n",
      "1  https://m.media-amazon.com/images/I/11TU2clswz...  11TU2clswzL.jpg\n",
      "2  https://m.media-amazon.com/images/I/11TU2clswz...  11TU2clswzL.jpg\n",
      "3  https://m.media-amazon.com/images/I/11TU2clswz...  11TU2clswzL.jpg\n",
      "4  https://m.media-amazon.com/images/I/11gHj8dhhr...  11gHj8dhhrL.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "test['image_filename'] = test['image_link'].apply(lambda x: os.path.basename(urlparse(x).path))\n",
    "\n",
    "# Display the dataframe to check the extracted filenames\n",
    "print(test[['image_link', 'image_filename']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d993bd-77a1-4615-aca0-4ab7fc6d3547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>image_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129022</th>\n",
       "      <td>129121</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81k5OuixnO...</td>\n",
       "      <td>603688</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>81k5OuixnOL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105015</th>\n",
       "      <td>105091</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61EO+zpRlu...</td>\n",
       "      <td>276611</td>\n",
       "      <td>width</td>\n",
       "      <td>61EO+zpRluL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95903</th>\n",
       "      <td>95977</td>\n",
       "      <td>https://m.media-amazon.com/images/I/619+qnYtZB...</td>\n",
       "      <td>174276</td>\n",
       "      <td>width</td>\n",
       "      <td>619+qnYtZBL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121218</th>\n",
       "      <td>121308</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71bB4LlhHG...</td>\n",
       "      <td>351022</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>71bB4LlhHGL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>4986</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41BXCQpIpx...</td>\n",
       "      <td>860821</td>\n",
       "      <td>wattage</td>\n",
       "      <td>41BXCQpIpxL.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                         image_link  group_id  \\\n",
       "129022  129121  https://m.media-amazon.com/images/I/81k5OuixnO...    603688   \n",
       "105015  105091  https://m.media-amazon.com/images/I/61EO+zpRlu...    276611   \n",
       "95903    95977  https://m.media-amazon.com/images/I/619+qnYtZB...    174276   \n",
       "121218  121308  https://m.media-amazon.com/images/I/71bB4LlhHG...    351022   \n",
       "4976      4986  https://m.media-amazon.com/images/I/41BXCQpIpx...    860821   \n",
       "\n",
       "        entity_name   image_filename  \n",
       "129022  item_weight  81k5OuixnOL.jpg  \n",
       "105015        width  61EO+zpRluL.jpg  \n",
       "95903         width  619+qnYtZBL.jpg  \n",
       "121218  item_weight  71bB4LlhHGL.jpg  \n",
       "4976        wattage  41BXCQpIpxL.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35215873-b3c3-49b7-8dd2-ffc6b896693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>image_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://m.media-amazon.com/images/I/110EibNycl...</td>\n",
       "      <td>156839</td>\n",
       "      <td>height</td>\n",
       "      <td>110EibNyclL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>width</td>\n",
       "      <td>11TU2clswzL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11gHj8dhhr...</td>\n",
       "      <td>792578</td>\n",
       "      <td>depth</td>\n",
       "      <td>11gHj8dhhrL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11lshEUmCr...</td>\n",
       "      <td>156839</td>\n",
       "      <td>height</td>\n",
       "      <td>11lshEUmCrL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>https://m.media-amazon.com/images/I/21+i52HRW4...</td>\n",
       "      <td>478357</td>\n",
       "      <td>width</td>\n",
       "      <td>21+i52HRW4L.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>https://m.media-amazon.com/images/I/310oxdFmgL...</td>\n",
       "      <td>913156</td>\n",
       "      <td>width</td>\n",
       "      <td>310oxdFmgLL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>141</td>\n",
       "      <td>https://m.media-amazon.com/images/I/310p+AOeZ6...</td>\n",
       "      <td>452717</td>\n",
       "      <td>height</td>\n",
       "      <td>310p+AOeZ6L.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>https://m.media-amazon.com/images/I/310qlaeUSA...</td>\n",
       "      <td>449805</td>\n",
       "      <td>depth</td>\n",
       "      <td>310qlaeUSAL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>https://m.media-amazon.com/images/I/310rX4WoAF...</td>\n",
       "      <td>704724</td>\n",
       "      <td>depth</td>\n",
       "      <td>310rX4WoAFL.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>https://m.media-amazon.com/images/I/310uxz1C1+...</td>\n",
       "      <td>825954</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>310uxz1C1+L.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                         image_link  group_id  \\\n",
       "0        0  https://m.media-amazon.com/images/I/110EibNycl...    156839   \n",
       "1        1  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "4        4  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
       "7        7  https://m.media-amazon.com/images/I/11lshEUmCr...    156839   \n",
       "8        8  https://m.media-amazon.com/images/I/21+i52HRW4...    478357   \n",
       "..     ...                                                ...       ...   \n",
       "139    139  https://m.media-amazon.com/images/I/310oxdFmgL...    913156   \n",
       "141    141  https://m.media-amazon.com/images/I/310p+AOeZ6...    452717   \n",
       "142    142  https://m.media-amazon.com/images/I/310qlaeUSA...    449805   \n",
       "144    144  https://m.media-amazon.com/images/I/310rX4WoAF...    704724   \n",
       "147    147  https://m.media-amazon.com/images/I/310uxz1C1+...    825954   \n",
       "\n",
       "     entity_name   image_filename  \n",
       "0         height  110EibNyclL.jpg  \n",
       "1          width  11TU2clswzL.jpg  \n",
       "4          depth  11gHj8dhhrL.jpg  \n",
       "7         height  11lshEUmCrL.jpg  \n",
       "8          width  21+i52HRW4L.jpg  \n",
       "..           ...              ...  \n",
       "139        width  310oxdFmgLL.jpg  \n",
       "141       height  310p+AOeZ6L.jpg  \n",
       "142        depth  310qlaeUSAL.jpg  \n",
       "144        depth  310rX4WoAFL.jpg  \n",
       "147  item_volume  310uxz1C1+L.jpg  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'image_id' column\n",
    "test_cleaned = test.drop_duplicates(subset='image_filename', keep='first')\n",
    "\n",
    "test_cleaned=test_cleaned.head(100)\n",
    "test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0220a02-6802-4ae8-a7a5-3e936fa5b0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'image_link', 'group_id', 'entity_name', 'image_filename'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "if test_cleaned['image_filename'].duplicated().any():\n",
    "    print(\"Warning: Duplicate image IDs found in CSV!\")\n",
    "test_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0896c515-ba7a-4cbc-8207-8f91372b7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 10:15:32.993289: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-16 10:15:33.202758: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 10:15:33.202790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 10:15:33.229829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 10:15:33.297425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 10:15:34.128398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import tensorflow as tf\n",
    "\n",
    "image_count = len(list(os.listdir('../images_test/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbb3bee-3d04-4b21-b253-3a9c8d683e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90666\n"
     ]
    }
   ],
   "source": [
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d9a4b9-6537-4fe7-846a-2cb6570a963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\n\\n# Define the image directory path\\nimage_directory = \"../Projects/images/\"\\n\\nimage_files_in_dir = set(os.listdir(image_directory))\\n\\n# Filter the DataFrame to keep only rows where image_filename exists in the directory\\ntrain_cleaned_filtered = train_cleaned[train_cleaned[\\'image_filename\\'].isin(image_files_in_dir)]\\n\\nfor new_index, row in enumerate(train_cleaned_filtered.itertuples(), start=1):\\n    old_image_name = row.image_filename\\n    old_image_path = os.path.join(image_directory, old_image_name)\\n\\n    # Create the new image name\\n    new_image_name = f\"{new_index}.jpg\"\\n    new_image_path = os.path.join(image_directory, new_image_name)\\n\\n    # Rename the image file in the directory\\n    os.rename(old_image_path, new_image_path)\\n\\n    # Update the DataFrame with the new image filename\\n    train_cleaned_filtered.at[row.Index, \"image_filename\"] = new_image_name'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the image directory path\n",
    "image_directory = \"../Projects/images/\"\n",
    "\n",
    "image_files_in_dir = set(os.listdir(image_directory))\n",
    "\n",
    "# Filter the DataFrame to keep only rows where image_filename exists in the directory\n",
    "train_cleaned_filtered = train_cleaned[train_cleaned['image_filename'].isin(image_files_in_dir)]\n",
    "\n",
    "for new_index, row in enumerate(train_cleaned_filtered.itertuples(), start=1):\n",
    "    old_image_name = row.image_filename\n",
    "    old_image_path = os.path.join(image_directory, old_image_name)\n",
    "\n",
    "    # Create the new image name\n",
    "    new_image_name = f\"{new_index}.jpg\"\n",
    "    new_image_path = os.path.join(image_directory, new_image_name)\n",
    "\n",
    "    # Rename the image file in the directory\n",
    "    os.rename(old_image_path, new_image_path)\n",
    "\n",
    "    # Update the DataFrame with the new image filename\n",
    "    train_cleaned_filtered.at[row.Index, \"image_filename\"] = new_image_name'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10c774d-60e6-4c32-8cc2-9bd4204f269b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Save the filtered DataFrame if needed\\ntrain_cleaned_filtered.to_csv(\\'filtered_dataframe.csv\\', index=False)\\n\\n\\nprint(\"Filtering completed. Remaining rows:\", len(train_cleaned_filtered))\\ntrain_cleaned_filtered'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Save the filtered DataFrame if needed\n",
    "train_cleaned_filtered.to_csv('filtered_dataframe.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Filtering completed. Remaining rows:\", len(train_cleaned_filtered))\n",
    "train_cleaned_filtered'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e3a162-93c8-41ca-8fc4-bbfef8e6b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv( 'filtered_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e20727-4e0b-4360-9714-4b60a86184ba",
   "metadata": {},
   "source": [
    "## For out of memory errors in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491400cf-6358-4b6c-a227-4769c5f1db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 10:15:48.414926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-16 10:15:48.516342: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices and set memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91aaa5e-9602-4e53-86b4-efc96a9e5777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca4307d-32e6-49b7-914d-9e15412d9d68",
   "metadata": {},
   "source": [
    "##IMAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1572fba-0c9b-47e0-be09-8218c6be93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "# List all image file paths from the directory\n",
    "image_dir = \"../images_test\"\n",
    "image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith(\".jpg\")]\n",
    "\n",
    "\n",
    "class CustomImageDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, batch_size, image_size, contrast_factor=1.1):\n",
    "        self.image_paths = image_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.contrast_factor = contrast_factor\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        # Convert to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Denoise using Non-Local Means Denoising\n",
    "        denoised_image = cv2.fastNlMeansDenoising(gray_image, None, 5, 7, 21)\n",
    "        \n",
    "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) for better contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(16, 16))\n",
    "        enhanced_contrast = clahe.apply(denoised_image)\n",
    "        \n",
    "        # Adaptive thresholding for better binary conversion in uneven lighting\n",
    "        binary_image = cv2.adaptiveThreshold(enhanced_contrast, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                             cv2.THRESH_BINARY, 11, 2)\n",
    "        \n",
    "        # Optional: Apply morphological operations to clean up small noise\n",
    "        kernel = np.ones((1, 1), np.uint8)\n",
    "        morphed_image = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_image = Image.fromarray(morphed_image)\n",
    "        \n",
    "        # Enhance contrast using PIL for fine-tuning\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        enhanced_image = enhancer.enhance(self.contrast_factor)\n",
    "        \n",
    "        return np.array(enhanced_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                image = cv2.imread(path)\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Unable to load image {path}\")\n",
    "                    continue\n",
    "                image = cv2.resize(image, self.image_size)  # Resize image\n",
    "                image = self.preprocess_image(image)  # Apply the preprocessing pipeline\n",
    "                \n",
    "                # Append to batch_images\n",
    "                batch_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {path}: {e}\")\n",
    "        \n",
    "        batch_images = np.array(batch_images)\n",
    "        return batch_images\n",
    "\n",
    "batch_size = 100\n",
    "image_size = (1536, 1536)  # Set image size to 1024x1024\n",
    "data_generator = CustomImageDataGenerator(image_paths, batch_size=batch_size, image_size=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b6598-df18-44e1-9851-c48af45a5a0a",
   "metadata": {},
   "source": [
    "## Splitting entity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eacd93f-107d-4a3d-9be9-caaef506051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image  # Ensure PIL is imported for image processing\n",
    "import os\n",
    "\n",
    "# Define the unit extraction map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'depth': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'height': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
    "    'item_weight': {'gram', 'g', 'gm', 'kilogram', 'kg', 'microgram', 'µg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton', 't'},\n",
    "    'maximum_weight_recommendation': {'gram', 'g', 'gm', 'kilogram', 'kg', 'microgram', 'µg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton', 't'},\n",
    "    'voltage': {'kilovolt', 'kV', 'millivolt', 'mV', 'volt', 'V'},\n",
    "    'wattage': {'kilowatt', 'kW', 'watt', 'W'},\n",
    "    'item_volume': {'centilitre', 'cL', 'cubic foot', 'ft³', 'cubic inch', 'in³', 'cup', 'c', 'decilitre', 'dL', 'fluid ounce', 'fl oz', 'gallon', 'gal', 'imperial gallon', 'imp gal', 'litre', 'L', 'microlitre', 'µL', 'millilitre', 'mL', 'pint', 'pt', 'quart', 'qt'}\n",
    "}\n",
    "\n",
    "def extract_value_and_unit(text, allowed_units):\n",
    "    value = None\n",
    "    unit = None\n",
    "    value_pattern = re.compile(r'\\d+(\\.\\d+)?')\n",
    "    unit_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(unit) for unit in allowed_units) + r')\\b', re.IGNORECASE)\n",
    "    value_match = value_pattern.search(text)\n",
    "    unit_match = unit_pattern.search(text)\n",
    "    if value_match:\n",
    "        value = value_match.group()\n",
    "    if unit_match:\n",
    "        unit = unit_match.group()\n",
    "    return value, unit\n",
    "\n",
    "def extract_entities_from_image(image, image_filename):\n",
    "    # Convert NumPy image to PIL format (if needed for pytesseract)\n",
    "    pil_image = Image.fromarray(image)\n",
    "\n",
    "    # Perform OCR\n",
    "    text = pytesseract.image_to_string(pil_image, output_type=Output.STRING)\n",
    "    \n",
    "    extracted_values = []\n",
    "    for entity, units in entity_unit_map.items():\n",
    "        value, unit = extract_value_and_unit(text, units)\n",
    "        if value and unit:\n",
    "            extracted_values.append(f'{value} {unit}')\n",
    "    \n",
    "    # Print the extracted values for debugging\n",
    "    print(f'Extracted from {image_filename}: {\", \".join(extracted_values)}')\n",
    "\n",
    "    return {\n",
    "        'image_filename': image_filename,\n",
    "        'Text': ', '.join(extracted_values)  # Combine all extracted values into a single string\n",
    "    }\n",
    "\n",
    "def process_batch(images, filenames):\n",
    "    data = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = executor.map(lambda image_file: extract_entities_from_image(*image_file), zip(images, filenames))\n",
    "        for result in results:\n",
    "            data.append(result)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22947de1-aeec-4b40-9529-4af17b87b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted from 11TU2clswzL.jpg: \n",
      "Extracted from 110EibNyclL.jpg: \n",
      "Extracted from 11gHj8dhhrL.jpg: \n",
      "Extracted from 11lshEUmCrL.jpg: \n",
      "Extracted from 21+i52HRW4L.jpg: 40 cm, 40 cm, 40 cm\n",
      "Extracted from 21-LmSmehZL.jpg: 40 cm, 40 cm, 40 cm\n",
      "Extracted from 213wY3gUsmL.jpg: \n",
      "Extracted from 214CLs1oznL.jpg: \n",
      "Extracted from 213oP6n7jtL.jpg: 4 M, 4 M, 4 M, 4 g, 4 g\n",
      "Extracted from 218BCzgKxuL.jpg: \n",
      "Extracted from 2174yonQBtL.jpg: \n",
      "Extracted from 216rjgJHAeL.jpg: 3 t, 3 t\n",
      "Extracted from 21aD6ktvwxS.jpg: \n",
      "Extracted from 21BMc5GC4iL.jpg: \n",
      "Extracted from 21bwWoCpGJL.jpg: \n",
      "Extracted from 21bfrFeArAL.jpg: \n",
      "Extracted from 21cLufe8Y5L.jpg: \n",
      "Extracted from 21CxlWbim3L.jpg: \n",
      "Extracted from 21d6Dtc94mL.jpg: 5.4 cm, 5.4 cm, 5.4 cm\n",
      "Extracted from 21fIe0wHxDL.jpg: \n",
      "Extracted from 21GLFXwC1mS.jpg: 185 cm, 185 cm, 185 cm\n",
      "Extracted from 21H+7R85YZL.jpg: \n",
      "Extracted from 21DZ7BAZ6-L.jpg: \n",
      "Extracted from 21Is45vdL0L.jpg: \n",
      "Extracted from 21jOOqo0oZL.jpg: 48 in, 48 in, 48 in\n",
      "Extracted from 21lqVGTzV1L.jpg: 60 in, 60 in, 60 in\n",
      "Extracted from 21m65YQrQdL.jpg: \n",
      "Extracted from 21IGmiJi-PL.jpg: \n",
      "Extracted from 21nEy7KBQlL.jpg: 47 cm, 47 cm, 47 cm\n",
      "Extracted from 21pFwZ8ghkL.jpg: \n",
      "Extracted from 21qcJWXFNkL.jpg: \n",
      "Extracted from 21qgWY60ABL.jpg: \n",
      "Extracted from 21qleENrBBL.jpg: \n",
      "Extracted from 21s+qly8nXL.jpg: 5.4 in, 5.4 in, 5.4 in\n",
      "Extracted from 21Raw7jSIML.jpg: \n",
      "Extracted from 21tOerzlVML.jpg: 14 inch, 14 inch, 14 inch, 14 C\n",
      "Extracted from 21vCDAVwSXS.jpg: \n",
      "Extracted from 21twwj6e-VL.jpg: \n",
      "Extracted from 21Vc5ixqKpS.jpg: \n",
      "Extracted from 21W7FvftSCL.jpg: 90 mm, 90 mm, 90 mm\n",
      "Extracted from 21vv80MKQEL.jpg: \n",
      "Extracted from 21xnCHoPZHL.jpg: \n",
      "Extracted from 21xX4tzOqKL.jpg: \n",
      "Extracted from 21ykCJJsDpL.jpg: \n",
      "Extracted from 21ztnqtFNVL.jpg: \n",
      "Extracted from 31+21njYcOL.jpg: 4.7 in, 4.7 in, 4.7 in\n",
      "Extracted from 31+1xiVjZBL.jpg: \n",
      "Extracted from 31+H+PVsNEL.jpg: \n",
      "Extracted from 31+B4SvByXL.jpg: \n",
      "Extracted from 31+KMtY+QXL.jpg: \n",
      "Extracted from 31+kevQrOnL.jpg: \n",
      "Extracted from 31+KPbkPVPL.jpg: \n",
      "Extracted from 31+NHijx4tL.jpg: \n",
      "Extracted from 31+nz2egbjL.jpg: \n",
      "Extracted from 31+sbhHNfYS.jpg: 15 mm, 15 mm, 15 mm\n",
      "Extracted from 31+qzYwhJ9L.jpg: \n",
      "Extracted from 31+RAxx7XaL.jpg: \n",
      "Extracted from 31+X1Lh3+eL.jpg: \n",
      "Extracted from 31+zGAugKrL.jpg: \n",
      "Extracted from 31-4a-yI8sS.jpg: \n",
      "Extracted from 31+zdbOuiTL.jpg: \n",
      "Extracted from 31-4uQ1S2XL.jpg: \n",
      "Extracted from 31-7OAd1COL.jpg: 3.8 cm, 3.8 cm, 3.8 cm\n",
      "Extracted from 31-8fLBjX8L.jpg: 14.9 in, 14.9 in, 14.9 in\n",
      "Extracted from 31-A90GRAHL.jpg: \n",
      "Extracted from 31-bBqUQPuL.jpg: 30 cm, 30 cm, 30 cm\n",
      "Extracted from 31-QiMl+gwL.jpg: 90 cm, 90 cm, 90 cm\n",
      "Extracted from 31-OBkMKDLL.jpg: \n",
      "Extracted from 31-Jdk3SMUL.jpg: \n",
      "Extracted from 31-Tp1rWTJL.jpg: \n",
      "Extracted from 310-VAupIJL.jpg: \n",
      "Extracted from 31-Ubz8YvUL.jpg: \n",
      "Extracted from 31-UedLQDqL.jpg: 73 cm, 73 cm, 73 cm\n",
      "Extracted from 31-W0zT17nL.jpg: \n",
      "Extracted from 3101lgy28BL.jpg: \n",
      "Extracted from 31014HApIqL.jpg: \n",
      "Extracted from 3105qskWRcL.jpg: \n",
      "Extracted from 3106iDqsfQL.jpg: \n",
      "Extracted from 3107VVwSyXL.jpg: \n",
      "Extracted from 3106Kd8KjnL.jpg: \n",
      "Extracted from 310aYU-JigL.jpg: \n",
      "Extracted from 310hyjPrgML.jpg: \n",
      "Extracted from 310GYVrrJPS.jpg: \n",
      "Extracted from 310dh1DQxRL.jpg: \n",
      "Extracted from 310kcUKMHOL.jpg: \n",
      "Extracted from 310mwtoWn3L.jpg: \n",
      "Extracted from 310BVU1bXvL.jpg: \n",
      "Extracted from 310p+AOeZ6L.jpg: \n",
      "Extracted from 310ICDHEK-L.jpg: \n",
      "Extracted from 310oxdFmgLL.jpg: \n",
      "Extracted from 310rX4WoAFL.jpg: \n",
      "Extracted from 310PNQ4FmqL.jpg: \n",
      "Extracted from 310Qevq+VSL.jpg: \n",
      "Extracted from 310qlaeUSAL.jpg: \n",
      "Extracted from 310Tz9gnk2L.jpg: 91 m, 91 m, 91 m\n",
      "Extracted from 310Th3tbqRL.jpg: \n",
      "Extracted from 310VuJknGFS.jpg: \n",
      "Extracted from 310X-ftA6SL.jpg: \n",
      "Extracted from 310YvVAYofL.jpg: \n",
      "Extracted from 310uxz1C1+L.jpg: \n",
      "Processed batch 1\n",
      "No matching images found for batch 2\n",
      "No matching images found for batch 3\n",
      "No matching images found for batch 4\n",
      "No matching images found for batch 5\n"
     ]
    }
   ],
   "source": [
    "def process_batches(data_generator, train_cleaned_filtered, batch_size=64, num_batches=None):\n",
    "    \"\"\"\n",
    "    Process a specified number of batches of preprocessed images.\n",
    "\n",
    "    Parameters:\n",
    "    - data_generator: Generator that yields preprocessed images.\n",
    "    - train_cleaned_filtered: DataFrame with image filenames and other data.\n",
    "    - batch_size: Number of images per batch (default: 64).\n",
    "    - num_batches: Number of batches to process (None means process all batches).\n",
    "    \"\"\"\n",
    "    total_batches = len(data_generator) if num_batches is None else min(num_batches, len(data_generator))\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        batch_images = data_generator[i]  # Get preprocessed images for this batch\n",
    "        batch_filenames = data_generator.image_paths[i * batch_size:(i + 1) * batch_size]  # Corresponding filenames\n",
    "        \n",
    "        # Filter filenames that are present in the dataset (consistent dataframe usage)\n",
    "        matching_filenames = [filename for filename in batch_filenames if os.path.basename(filename) in train_cleaned_filtered['image_filename'].values]\n",
    "        matching_images = [image for image, filename in zip(batch_images, batch_filenames) if os.path.basename(filename) in train_cleaned_filtered['image_filename'].values]\n",
    "        \n",
    "        # Check for empty batches\n",
    "        if len(matching_images) == 0:\n",
    "            print(f\"No matching images found for batch {i + 1}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract text from the batch of matching images\n",
    "        new_data = process_batch(matching_images, [os.path.basename(path) for path in matching_filenames])\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Update train_cleaned_filtered based on new_df\n",
    "        for _, row in new_df.iterrows():\n",
    "            image_filename = row['image_filename']\n",
    "            text = row['Text']\n",
    "\n",
    "            # Update the 'Text' field for matching image filenames\n",
    "            if image_filename in test_cleaned['image_filename'].values:\n",
    "                test_cleaned.loc[test_cleaned['image_filename'] == image_filename, 'Text'] = text\n",
    "\n",
    "        print(f'Processed batch {i + 1}')\n",
    "        \n",
    "        # Save the updated dataframe after each batch\n",
    "        test_cleaned.to_csv('updated_test_cleaned_filtered.csv', index=False)  # Save after each batch\n",
    "\n",
    "# Example usage\n",
    "num_batches_to_process = 5  # Change this to the number of batches you want to process\n",
    "process_batches(data_generator, test_cleaned, batch_size=100, num_batches=num_batches_to_process)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
